---
title: "Statistical Learning Final Project"
format:
  html:
    embed-resources: true
editor: visual
author: "Katrin Sauter & Jan Thiele"
date: 2026-01-26
---

# Load Packages and Dataset

```{r}
#| label: loading packages
#| message: false
#| warning: false

library(tidyverse)  
library(keras3)
library(psych)
library(corrplot)
library(ggplot2)

# Read the data
df <- read.csv("apple_quality.csv")
```

# Data Cleaning

```{r}
#| label: clean-data
#| message: false
#| warning: false

#check variables to get a first glance
summary(df)
# acidity is a character variable
df$Acidity <- as.numeric(df$Acidity)

# target variable needs to binary encoded because it is also a character
df$Quality <- case_match(df$Quality,
                         'bad'~0,
                         'good'~1)

#delete NAs, the last row is empty
df <- na.omit(df)

#check dimennsions 
print(dim(df))
```

# Exploratory Analysis

```{r, fig.width=6,fig.height=6}
#| label: Exploratory analysis
#| message: false
#| warning: false

# Define feature columns and target
# We only take the relevant features and exclude the fruits id, 
# since it has no predictive value
feats <- c("Size", "Weight", "Sweetness", "Crunchiness", "Juiciness", 
           "Ripeness", "Acidity")
predict <- "Quality"


# distribution of target 
ggplot(df, aes(x = factor(Quality))) +
  geom_bar(fill =c("#A3C3DC", "#3E72AD"))+
  ylim(0,2500)+
  scale_x_discrete(labels = c("bad", "good")) +
  ylab("Counts")+
  xlab("Apple Quality")+
  theme_minimal()
    
#target variable is perfectly split into 50:50

# summary statistics 
feat_describtives <- tibble(describe(df[,feats]))
feat_describtives$vars = feats
feat_describtives

#correlation matrix of features
co_matrix <- round(cor(df[, feats]),2)
corrplot(co_matrix, method = 'color', type ='lower', col = COL1('Blues', 10), 
         addCoef.col = 'white', tl.col = 'black', tl.srt = 45)


```

# Neural Network

## Preprocessing

```{r}
#| label: preprocessing-neural-network
#| message: false
#| warning: false

# train-test split
set.seed(123)
idx <- sample(seq_len(nrow(df)), size = 0.8*nrow(df))
train <- df[idx, ]
test  <- df[-idx, ]

c(number_train_set = nrow(train), number_test_set = nrow(test))


# outlier analysis 
#scale to findn values which are more than 3 sds from the mean and remove them
z <- scale(train[, feats])
outliers_row <- which(apply(abs(z) > 3, 1, any))
train <- train[-outliers_row, ]

#scale features in training data
train_scaled <- scale(train[, feats])

#dont know if necessary but we learned it this way in deep learning
#scale test set with train as mean and sd center
#save the scaled data into the orginal dataframe to use it for logistic baseline
train[feats] <- train_scaled
test[feats] <- scale(test[, feats],
                      center = attr(train_scaled, "scaled:center"),
                      scale  = attr(train_scaled, "scaled:scale"))

#get features and target train/ test sets
X_train <- as.matrix(train[, feats]) # input needs to be a matrix
y_train <- train[,predict]

X_test <- as.matrix(test[,feats])
y_test <- test[,predict]

```

## Model Architecture and Training

```{r}
#| label: neural network architecture and training
#| message: false
#| warning: false


set.seed(42) #reproducibiltiy
#model architecture
#three hidden layers with 7 - 128 - 64 - 32 - 1 with a sigmoid activation function in the output layer for binary classification
#added batch normalization and dropout rate if 20% after each layer to regularization to the network
model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = ncol(X_train)) %>%
  layer_batch_normalization()%>%
  layer_dropout(rate=0.2)%>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_batch_normalization()%>%
  layer_dropout(rate=0.2)%>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization()%>%
  layer_dropout(rate=0.2)%>%
  layer_dense(units = 1, activation = "sigmoid") %>%
  
  compile(
    loss = 'binary_crossentropy', # binary cross entropy loss for binary classfication
    optimizer = optimizer_adam(learning_rate = 0.001), 
    metrics = c('accuracy') #evaluation metrics, we included more to have wide variety to choose from in comparing it to the baseline
  )

# train the model 
fit1 <- model%>%
  fit(
    x = X_train,
    y = y_train,
    epochs = 100,#100 epochs because we got 20 epochs patience, give enough time to learn pattern
    batch_size = 128,
    validation_split = 0.2,#validation 20% to monitor training 
    verbose = FALSE, 
    callbacks = list(
      callback_early_stopping(patience = 20),#stop after 20 epochs of not improvment
      callback_reduce_lr_on_plateau(factor=0.05)#learning decrease on plateau
    )
  )
```

```{r}
#| label: Training visuals
#| message: false
#| warning: false
plot(fit1)+
  theme_minimal()+
  ylab('Evaluation Metrics')+
  xlab('Epoch')
```

## Neural Network Evaluation

```{r}
#| label: neural network evalution on test set
#| message: false
#| warning: false

# calculate loss 
results <- model %>% evaluate(X_test, y_test, batch_size=128) #evaluate with same batch size
loss_nn <- results$loss

#get actual predictions
predictions <- model %>% predict(X_test)
pred_class <- ifelse(predictions > 0.5, 1, 0) #threshold can be at 0.5 here because the target is 50:50 balanced

#assesing model
cm <- table(Predicted = pred_class, Actual = y_test)#confusion matrix

#extract values
TN <- cm["0","0"]
FP <- cm["1","0"]
FN <- cm["0","1"]
TP <- cm["1","1"]

#calculate metrics
accuracy_nn    <- (TP + TN) / sum(cm)
precision_nn   <- TP / (TP + FP)
recall_nn     <- TP / (TP + FN)
f1_nn        <- 2 * (precision_nn * recall_nn) / (precision_nn + recall_nn)

#combine into dataframe
data.frame(accuracy_nn, precision_nn, recall_nn, f1_nn, loss_nn)

# confusion matrix 
cm_df <- as.data.frame( table(Predicted = pred_class, Actual = y_test)) 

ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Freq)) +
    geom_tile(color = "white") +
    geom_text(aes(label = Freq), size = 5) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    scale_x_discrete(labels = c("bad", "good")) +
    scale_y_discrete(labels = c("bad", "good")) +
    ylab("Actual Quality")+
    xlab("Predicted Quality")+
    labs(fill = "Count") +
    theme_minimal(base_size = 12)
```

# Logistic Regression baseline model

```{r}
#| label: logistic-regression-baseline-model
#| message: false
#| warning: false

#logisitc regression formular
formular <- Quality ~ Size+Weight+Sweetness+Crunchiness+Juiciness+Ripeness+Acidity

# Fit logistic regression model
log_model <- glm(formular, data = train, family = binomial)

#assesing model
pred_probs <- predict(log_model, test, type = "response") #get prediction
pred_class <- ifelse(pred_probs > 0.5, 1, 0) #convert to classes

cm <- table(Predicted = pred_class, Actual = test$Quality)#confusion matrix

#extract values
TN <- cm["0","0"]
FP <- cm["1","0"]
FN <- cm["0","1"]
TP <- cm["1","1"]

#calculate metrics
accuracy_lg    <- (TP + TN) / sum(cm)
precision_lg   <- TP / (TP + FP)
recall_lg      <- TP / (TP + FN)
f1_lg          <- 2 * (precision_lg * recall_lg) / (precision_lg + recall_lg)

# calculate log loss
#Binary cross-entropy / log loss helper
log_loss <- function(y, p, eps = 1e-15) {
  p <- pmin(pmax(p, eps), 1 - eps)
  -mean(y * log(p) + (1 - y) * log(1 - p))
}
loss_lg <- log_loss(test$Quality, pred_probs)

#combine into dataframe
data.frame(accuracy_lg, precision_lg, recall_lg, f1_lg, loss_lg)

#visualize confusion matrix
ggplot(as.data.frame(cm) , aes(x = Predicted, y = Actual, fill = Freq)) +
    geom_tile(color = "white") +
    geom_text(aes(label = Freq), size = 5) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    scale_x_discrete(labels = c("bad", "good")) +
    scale_y_discrete(labels = c("bad", "good")) +
    ylab("Actual Quality")+
    xlab("Predicted Quality")+
    labs(fill = "Count") +
    theme_minimal(base_size = 12)


#coefficents and log odds
coefs <- summary(log_model)$coefficients

coef_df <- data.frame(
  Name = rownames(coefs),
  Coef = coefs[, "Estimate"],
  row.names = NULL
)

coef_df$Odds <- exp(coef_df$Coef)

coef_df

```

# Summary

```{r}
# summarize all the test performances into one dataframe

performance <- tibble("model" = c("Neural Network", "Logistic Regression"),
                          "Accuracy" = c(accuracy_nn, accuracy_lg), 
                          "Precision" = c(precision_nn, precision_lg), 
                          "Recall" = c(recall_nn,recall_lg), 
                          "F1-Score" = c(f1_nn, f1_lg), 
                          "Log-Loss" = c(loss_nn, loss_lg))

performance


```

Conclusion: Using a Neural Network led to a greater predictive performance across all metrics justifying it's use in this particular task.

# 
